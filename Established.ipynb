{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing of ExpPool Layer on Established Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from Abstract import Histogram as AHist\n",
    "\n",
    "def restrict_GPU_pytorch(gpuid, use_cpu=False):\n",
    "    \"\"\"\n",
    "        gpuid: str, comma separated list \"0\" or \"0,1\" or even \"0,1,3\"\n",
    "    \"\"\"\n",
    "    if not use_cpu:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpuid\n",
    "\n",
    "        print(\"Using GPU:{}\".format(gpuid))\n",
    "    else:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "        print(\"Using CPU\")\n",
    "        \n",
    "        \n",
    "gpuid = \"0\"\n",
    "use_cpu = True\n",
    "restrict_GPU_pytorch(gpuid, use_cpu=use_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_set = torchvision.datasets.CIFAR10(root=\"./cifardata\", train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root=\"./cifardata\", train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', \n",
    "           'car', \n",
    "           'bird',\n",
    "           'cat',\n",
    "           'deer', \n",
    "           'dog', \n",
    "           'frog', \n",
    "           'horse',\n",
    "           'ship',\n",
    "           'truck')\n",
    "\n",
    "#Training\n",
    "n_training_samples = 20000\n",
    "train_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n",
    "def get_train_loader(batch_size):\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
    "        return train_loader\n",
    "    \n",
    "#Validation\n",
    "n_val_samples = 5000\n",
    "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples+n_val_samples, dtype=np.int64))\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler, num_workers=2)\n",
    "\n",
    "#Testing\n",
    "n_test_samples = 5000\n",
    "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, sampler=test_sampler, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, resume_from = None):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    # The model (nn.Module) to return\n",
    "    model_ft = None\n",
    "    # The input image is expected to be (input_size, input_size)\n",
    "    input_size = 0\n",
    "    \n",
    "    # You may NOT use pretrained models!! \n",
    "    use_pretrained = False\n",
    "    \n",
    "    # By default, all parameters will be trained (useful when you're starting from scratch)\n",
    "    # Within this function you can set .requires_grad = False for various parameters, if you\n",
    "    # don't want to learn them\n",
    "    class Flatten(torch.nn.Module):\n",
    "        def forward(self, x):\n",
    "            batch_size = x.shape[0]\n",
    "            return x.view(batch_size, -1)\n",
    "\n",
    "    if model_name == \"resnet18\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "#         dropout = nn.Dropout()\n",
    "#         flatten = Flatten()\n",
    "#         hist = AHist.HistPool(num_bins=10)\n",
    "#         layers = list(model_ft.children())\n",
    "#         layers.insert(-1, hist)\n",
    "#         del layers[-1]\n",
    "#         del layers[-2]\n",
    "#         model_ft = nn.Sequential(*layers)\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        print(model_ft.children())\n",
    "        input_size = 224\n",
    "    \n",
    "    elif model_name == \"resnet50\":\n",
    "        \"\"\" Resnet50\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet50(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        # dropout = nn.Dropout(p=.2)\n",
    "        # flatten = Flatten()\n",
    "        # layers = list(model_ft.children())\n",
    "        # layers.insert(-1, flatten)\n",
    "        # layers.insert(-1, dropout)\n",
    "        # del layers[-1]\n",
    "        # model_ft = nn.Sequential(*layers)\n",
    "        # print(list(model_ft.children()))\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"resnet152\":\n",
    "        \"\"\" Resnet152\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet152(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        dropout = nn.Dropout()\n",
    "        flatten = Flatten()\n",
    "        layers = list(model_ft.children())\n",
    "        layers.insert(-1, flatten)\n",
    "        layers.insert(-1, dropout)\n",
    "        del layers[-1]\n",
    "        model_ft = nn.Sequential(*layers)\n",
    "        # print(list(model_ft.children()))\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Invalid model name!\")\n",
    "    \n",
    "    if resume_from is not None:\n",
    "        print(\"Loading weights from %s\" % resume_from)\n",
    "        x = torch.load(resume_from)\n",
    "        model_ft.load_state_dict(x)\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "# initialize_model(\"resnet18\", 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def loss_optimizer_scheduler(net, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Initializes the loss optimizer functions\n",
    "    \"\"\"\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=0, verbose=True)\n",
    "    \n",
    "    return loss, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "    \n",
    "    #Print all of the hyperparameters of the training iteration:\n",
    "    print(\"===== HYPERPARAMETERS =====\")\n",
    "    print(\"batch_size =\", batch_size)\n",
    "    print(\"epochs =\", n_epochs)\n",
    "    print(\"learning_rate =\", learning_rate)\n",
    "#     print(\"num_bins =\", list(model.children())[8].num_bins)\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    validation_losses = []\n",
    "    training_losses = []\n",
    "    \n",
    "    #Retrieve training data\n",
    "    train_loader = get_train_loader(batch_size)\n",
    "    n_batches = len(train_loader)\n",
    "    \n",
    "    #Initialize loss and optimizer functions\n",
    "    loss, optimizer, scheduler = loss_optimizer_scheduler(net, learning_rate)\n",
    "    \n",
    "    \n",
    "    training_start_time = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        print_every = n_batches // 10\n",
    "#         print_every = 1 if print_every == 0 else print_every\n",
    "        start_time = time.time()\n",
    "        total_train_loss = 0\n",
    "        \n",
    "        for i, data in tqdm(enumerate(train_loader, 0)):\n",
    "            \n",
    "            #Get inputs\n",
    "            inputs, labels = data\n",
    "#             if epoch == 0:\n",
    "#                 global img\n",
    "#                 img = inputs\n",
    "#                 target = inputs[0]\n",
    "#                 imshow(target)\n",
    "#                 print(classes[labels[0]])\n",
    "            inputs, labels = Variable(inputs), Variable(labels)   \n",
    "                        \n",
    "            #Set parameter gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #Forward pass, backward pass, optimize\n",
    "            outputs = net(inputs)            \n",
    "            loss_size = loss(outputs, labels)\n",
    "            \n",
    "            #Intermediate accuracy\n",
    "#             _, predicted = torch.max(outputs.data, dim=1)\n",
    "#             num_correct = (predicted == labels).sum().item()\n",
    "#             intermediate_acc = (num_correct * 100.0 / labels.size(0))\n",
    "            \n",
    "            loss_size.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            #Update statistics\n",
    "            running_loss += loss_size.data.item()\n",
    "            total_train_loss += loss_size.data.item()\n",
    "            \n",
    "            \n",
    "            #Print statistics every 10th batch of epoch\n",
    "            if (i+1) % (print_every+1) == 0:\n",
    "                print(\"Epoch {epoch}, {percent_complete_epoch:d}% \\t train_loss: {train_loss:.2f} \\t took: {time:.2f}s\".format(\n",
    "                    epoch = epoch+1, \n",
    "                    percent_complete_epoch = int(100 * (i+1) / n_batches), \n",
    "                    train_loss = running_loss / print_every, \n",
    "                    time = time.time() - start_time, \n",
    "#                     accuracy = intermediate_acc.item()\n",
    "                ))\n",
    "                \n",
    "                #Reset running loss and time\n",
    "                running_loss = 0.0\n",
    "                start_time = time.time()\n",
    "            \n",
    "        #After each epoch, run a pass on validation set\n",
    "        total_val_loss = 0\n",
    "        \n",
    "        for inputs, labels in val_loader:\n",
    "            \n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \n",
    "            #Forward pass\n",
    "            val_outputs = net(inputs)\n",
    "            val_loss_size = loss(val_outputs, labels)\n",
    "            total_val_loss += val_loss_size.data.item()\n",
    "        \n",
    "        print(\"Validation loss = {:.2f}\".format(total_val_loss / len(val_loader)))\n",
    "        \n",
    "        validation_losses.append(total_val_loss / len(val_loader))\n",
    "        training_losses.append(total_train_loss / len(train_loader))\n",
    "        scheduler.step(total_val_loss / len(val_loader))\n",
    "        \n",
    "        \n",
    "        \n",
    "    print(\"=\" * 30)\n",
    "    print(\"Training finished, took {:.2f}s\".format(time.time() - training_start_time))\n",
    "    \n",
    "    plot_loss(n_epochs=n_epochs, \n",
    "              training_losses=training_losses, \n",
    "              validation_losses=validation_losses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.children at 0x7fb56a7cd4c0>\n",
      "===== HYPERPARAMETERS =====\n",
      "batch_size = 32\n",
      "epochs = 5\n",
      "learning_rate = 0.001\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcd687b6d87430a910b5a5dfe67507c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, 10% \t train_loss: 2.11 \t took: 23.05s\n",
      "Epoch 1, 20% \t train_loss: 1.81 \t took: 22.90s\n",
      "Epoch 1, 30% \t train_loss: 1.67 \t took: 22.64s\n",
      "Epoch 1, 40% \t train_loss: 1.68 \t took: 24.04s\n",
      "Epoch 1, 50% \t train_loss: 1.63 \t took: 26.93s\n",
      "Epoch 1, 60% \t train_loss: 1.55 \t took: 24.81s\n",
      "Epoch 1, 70% \t train_loss: 1.58 \t took: 30.37s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bb7cee9aaa92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"models/TEST\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0cded4763f5f>\u001b[0m in \u001b[0;36mtrainNet\u001b[0;34m(net, batch_size, n_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#             intermediate_acc = (num_correct * 100.0 / labels.size(0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mloss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "model_name = \"resnet18\"\n",
    "num_classes = 10\n",
    "resume_from = None\n",
    "model, input_size = initialize_model(model_name = model_name, num_classes = num_classes, resume_from = resume_from)\n",
    "\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "learning_rate = 1e-3\n",
    "\n",
    "trainNet(model, batch_size=batch_size, n_epochs=n_epochs, learning_rate=learning_rate)\n",
    "\n",
    "torch.save(model, \"models/TEST\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
